{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sri_S\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 932ms/step - accuracy: 0.1197 - loss: 3.2689 - val_accuracy: 0.1563 - val_loss: 2.2910 - learning_rate: 3.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 915ms/step - accuracy: 0.1216 - loss: 2.9138 - val_accuracy: 0.0820 - val_loss: 2.5289 - learning_rate: 3.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823ms/step - accuracy: 0.1290 - loss: 2.8199\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 917ms/step - accuracy: 0.1290 - loss: 2.8197 - val_accuracy: 0.0820 - val_loss: 2.5034 - learning_rate: 3.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 839ms/step - accuracy: 0.1317 - loss: 2.7152 - val_accuracy: 0.0820 - val_loss: 2.3589 - learning_rate: 1.5000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758ms/step - accuracy: 0.1157 - loss: 2.7070\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 855ms/step - accuracy: 0.1158 - loss: 2.7067 - val_accuracy: 0.1650 - val_loss: 2.4703 - learning_rate: 1.5000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 872ms/step - accuracy: 0.1228 - loss: 2.6600 - val_accuracy: 0.0885 - val_loss: 2.3987 - learning_rate: 7.5000e-05\n",
      "Epoch 1/5\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1158s\u001b[0m 7s/step - accuracy: 0.1179 - loss: 3.2166 - val_accuracy: 0.1519 - val_loss: 2.3994 - learning_rate: 5.0000e-05\n",
      "Epoch 2/5\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 3s/step - accuracy: 0.1304 - loss: 2.8966 - val_accuracy: 0.0885 - val_loss: 2.9749 - learning_rate: 5.0000e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.1331 - loss: 2.9129\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m714s\u001b[0m 4s/step - accuracy: 0.1331 - loss: 2.9129 - val_accuracy: 0.1246 - val_loss: 2.6993 - learning_rate: 5.0000e-05\n",
      "Epoch 4/5\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 3s/step - accuracy: 0.1597 - loss: 2.8103 - val_accuracy: 0.1005 - val_loss: 2.9828 - learning_rate: 2.5000e-05\n",
      "Epoch 5/5\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.1408 - loss: 2.8723\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 2s/step - accuracy: 0.1408 - loss: 2.8723 - val_accuracy: 0.0962 - val_loss: 2.9146 - learning_rate: 2.5000e-05\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 436ms/step - accuracy: 0.2426 - loss: 2.3153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 18.46% | Test Loss: 2.3706\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Data paths\n",
    "base_path = r'D:\\fake product\\Data'\n",
    "train_path = os.path.join(base_path, 'train')\n",
    "test_path = os.path.join(base_path, 'test')\n",
    "\n",
    "# Enhanced preprocessing with CLAHE\n",
    "def preprocess_images(dataset_path, image_size=(224, 224)):\n",
    "    images, labels = [], []\n",
    "    classes = sorted(os.listdir(dataset_path))\n",
    "    class_map = {label: idx for idx, label in enumerate(classes)}\n",
    "    \n",
    "    for label in classes:\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        for img_name in os.listdir(label_path):\n",
    "            img_path = os.path.join(label_path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                # Apply CLAHE for contrast enhancement\n",
    "                lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "                l, a, b = cv2.split(lab)\n",
    "                clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "                l = clahe.apply(l)\n",
    "                lab = cv2.merge((l, a, b))\n",
    "                img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "                \n",
    "                img = cv2.resize(img, image_size)\n",
    "                images.append(img)\n",
    "                labels.append(class_map[label])\n",
    "    \n",
    "    return np.array(images) / 255.0, np.array(labels), class_map\n",
    "\n",
    "# Load Data\n",
    "X_train, y_train, class_map = preprocess_images(train_path, (224, 224))\n",
    "X_test, y_test, _ = preprocess_images(test_path, (224, 224))\n",
    "\n",
    "# Split Data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, stratify=y_train, random_state=42)\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(class_map)\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Class Weights for Balanced Training\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Augment Training Data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train_cat, batch_size=32)\n",
    "\n",
    "# Hybrid Model: EfficientNetB0 + CNN\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze base model layers\n",
    "\n",
    "# Add Custom CNN Layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=l2(0.0005))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(0.0005))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Build Model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=AdamW(learning_rate=0.0003), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for Faster Convergence\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=2, verbose=1)\n",
    "]\n",
    "\n",
    "# Train Model (Reduced Epochs)\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fine-tune by Unfreezing EfficientNet Layers\n",
    "base_model.trainable = True\n",
    "model.compile(optimizer=AdamW(learning_rate=0.00005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue Training (5 More Epochs for Fine-tuning)\n",
    "history_finetune = model.fit(\n",
    "    train_generator, \n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=5, \n",
    "    batch_size=32, \n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate Model on Test Set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}% | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Save Model\n",
    "model.save('hybrid_model_fake_product.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
